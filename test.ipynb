{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed9f07b7ee84fcabd04ec27c097c2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/baojiedama/moh/test.ipynb 单元格 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/baojiedama/moh/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m SETTING \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m       \u001b[39m# 可选\"default\", \"single\", \"student-forcing\", \"teacher-forcing\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/baojiedama/moh/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m DATA_JSON \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DATASET_PATH, \u001b[39m\"\u001b[39m\u001b[39mmerged_mixed_data.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/baojiedama/moh/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m model_handler \u001b[39m=\u001b[39m LlavaHFHandler(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/baojiedama/moh/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllava-hf\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/baojiedama/moh/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     model_size\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m7b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/baojiedama/moh/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     model_path\u001b[39m=\u001b[39;49mMODEL_PATH,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/baojiedama/moh/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/baojiedama/moh/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/baojiedama/moh/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m image_processor \u001b[39m=\u001b[39m ImageProcessor()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/baojiedama/moh/test.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# 评测类\u001b[39;00m\n",
      "File \u001b[0;32m/home/baojiedama/moh/model_handler.py:171\u001b[0m, in \u001b[0;36mLlavaHFHandler.__init__\u001b[0;34m(self, model_name, model_size, model_path, device_map)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name \u001b[39m=\u001b[39m model_name\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_size \u001b[39m=\u001b[39m model_size\n\u001b[0;32m--> 171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_model(model_path, device_map)\n",
      "File \u001b[0;32m/home/baojiedama/moh/model_handler.py:175\u001b[0m, in \u001b[0;36mLlavaHFHandler.init_model\u001b[0;34m(self, model_path, device_map)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minit_model\u001b[39m(\u001b[39mself\u001b[39m, model_path, device_map):\n\u001b[1;32m    174\u001b[0m     processor \u001b[39m=\u001b[39m AutoProcessor\u001b[39m.\u001b[39mfrom_pretrained(model_path, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, use_fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 175\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForVision2Seq\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    176\u001b[0m         model_path,\n\u001b[1;32m    177\u001b[0m         torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[1;32m    178\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    179\u001b[0m         trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    180\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m processor, model\n",
      "File \u001b[0;32m/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[39mif\u001b[39;00m model_class\u001b[39m.\u001b[39mconfig_class \u001b[39m==\u001b[39m config\u001b[39m.\u001b[39msub_configs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtext_config\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    573\u001b[0m     )\n\u001b[1;32m    574\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/transformers/modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m old_dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mget_default_dtype()\n\u001b[1;32m    308\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    310\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     torch\u001b[39m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/transformers/modeling_utils.py:4574\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4564\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4565\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4567\u001b[0m     (\n\u001b[1;32m   4568\u001b[0m         model,\n\u001b[1;32m   4569\u001b[0m         missing_keys,\n\u001b[1;32m   4570\u001b[0m         unexpected_keys,\n\u001b[1;32m   4571\u001b[0m         mismatched_keys,\n\u001b[1;32m   4572\u001b[0m         offload_index,\n\u001b[1;32m   4573\u001b[0m         error_msgs,\n\u001b[0;32m-> 4574\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   4575\u001b[0m         model,\n\u001b[1;32m   4576\u001b[0m         state_dict,\n\u001b[1;32m   4577\u001b[0m         checkpoint_files,\n\u001b[1;32m   4578\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   4579\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   4580\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   4581\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   4582\u001b[0m         disk_offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   4583\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   4584\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   4585\u001b[0m         hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   4586\u001b[0m         keep_in_fp32_regex\u001b[39m=\u001b[39;49mkeep_in_fp32_regex,\n\u001b[1;32m   4587\u001b[0m         device_mesh\u001b[39m=\u001b[39;49mdevice_mesh,\n\u001b[1;32m   4588\u001b[0m         key_mapping\u001b[39m=\u001b[39;49mkey_mapping,\n\u001b[1;32m   4589\u001b[0m         weights_only\u001b[39m=\u001b[39;49mweights_only,\n\u001b[1;32m   4590\u001b[0m     )\n\u001b[1;32m   4592\u001b[0m \u001b[39m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4593\u001b[0m model\u001b[39m.\u001b[39m_tp_size \u001b[39m=\u001b[39m tp_size\n",
      "File \u001b[0;32m/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/transformers/modeling_utils.py:5031\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5029\u001b[0m \u001b[39m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m   5030\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_fsdp_enabled() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_local_dist_rank_0() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_quantized):\n\u001b[0;32m-> 5031\u001b[0m     disk_offload_index, cpu_offload_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   5032\u001b[0m         model_to_load,\n\u001b[1;32m   5033\u001b[0m         state_dict,\n\u001b[1;32m   5034\u001b[0m         shard_file,\n\u001b[1;32m   5035\u001b[0m         expected_keys,\n\u001b[1;32m   5036\u001b[0m         reverse_key_renaming_mapping,\n\u001b[1;32m   5037\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   5038\u001b[0m         disk_offload_folder\u001b[39m=\u001b[39;49mdisk_offload_folder,\n\u001b[1;32m   5039\u001b[0m         disk_offload_index\u001b[39m=\u001b[39;49mdisk_offload_index,\n\u001b[1;32m   5040\u001b[0m         cpu_offload_folder\u001b[39m=\u001b[39;49mcpu_offload_folder,\n\u001b[1;32m   5041\u001b[0m         cpu_offload_index\u001b[39m=\u001b[39;49mcpu_offload_index,\n\u001b[1;32m   5042\u001b[0m         hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   5043\u001b[0m         is_safetensors\u001b[39m=\u001b[39;49mis_offloaded_safetensors,\n\u001b[1;32m   5044\u001b[0m         keep_in_fp32_regex\u001b[39m=\u001b[39;49mkeep_in_fp32_regex,\n\u001b[1;32m   5045\u001b[0m         unexpected_keys\u001b[39m=\u001b[39;49munexpected_keys,\n\u001b[1;32m   5046\u001b[0m         device_mesh\u001b[39m=\u001b[39;49mdevice_mesh,\n\u001b[1;32m   5047\u001b[0m     )\n\u001b[1;32m   5049\u001b[0m \u001b[39m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[1;32m   5050\u001b[0m \u001b[39mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/transformers/modeling_utils.py:843\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[39mif\u001b[39;00m is_fsdp_enabled():\n\u001b[1;32m    841\u001b[0m         param_device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m is_local_dist_rank_0() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 843\u001b[0m     _load_parameter_into_model(model, param_name, param\u001b[39m.\u001b[39;49mto(param_device))\n\u001b[1;32m    845\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    846\u001b[0m     hf_quantizer\u001b[39m.\u001b[39mcreate_quantized_param(\n\u001b[1;32m    847\u001b[0m         model, param, param_name, param_device, state_dict, unexpected_keys\n\u001b[1;32m    848\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from image_processor import ImageProcessor\n",
    "from model_handler import LlavaHFHandler\n",
    "from evaluation import  get_evaluation\n",
    "\n",
    "MODEL_PATH = \"llava-hf/llava-1.5-7b-hf\"\n",
    "DATASET_PATH = \"/home/baojiedama/ROPE/validation/\"\n",
    "DATA_TYPE = \"validation\"  # 'train' or 'validation'\n",
    "SETTING = \"default\"       # 可选\"default\", \"single\", \"student-forcing\", \"teacher-forcing\"\n",
    "DATA_JSON = os.path.join(DATASET_PATH, \"merged_mixed_data.json\")\n",
    "\n",
    "model_handler = LlavaHFHandler(\n",
    "    model_name=\"llava-hf\",\n",
    "    model_size=\"7b\",\n",
    "    model_path=MODEL_PATH,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "image_processor = ImageProcessor()\n",
    "\n",
    "# 评测类\n",
    "EvalClass = get_evaluation(SETTING)\n",
    "evaluator = EvalClass(model_handler, image_processor, DATASET_PATH, DATA_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 加载数据 ==========\n",
    "import json\n",
    "with open(DATA_JSON, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "acc_list = [0] * 5\n",
    "photo2answer = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 主评测循环 ==========\n",
    "for entry in dataset:\n",
    "    evaluator.process_entry(entry, acc_list, photo2answer)\n",
    "\n",
    "# ========== 评测指标输出 ==========\n",
    "metrics = evaluator.calculate_metrics(acc_list)\n",
    "import pprint\n",
    "pprint.pprint(metrics)\n",
    "\n",
    "# ========== 保存结果 ==========\n",
    "with open(\"moh_llava_results.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "with open(\"moh_llava_pred.json\", \"w\") as f:\n",
    "    json.dump(photo2answer, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
