{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3c111fcd3a4fb3bfb0f7627338f101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from image_processor import ImageProcessor\n",
    "from model_handler import LlavaNextHandler\n",
    "from evaluation import  get_evaluation\n",
    "\n",
    "MODEL_PATH = \"/llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "DATASET_PATH = \"/home/baojiedama/ROPE\"\n",
    "DATA_TYPE = \"validation\"  # 'train' or 'validation'\n",
    "SETTING = \"default\"       # 可选\"default\", \"single\", \"student-forcing\", \"teacher-forcing\"\n",
    "DATA_JSON = os.path.join(DATASET_PATH, \"merged_mixed_data.json\")\n",
    "\n",
    "model_handler = LlavaNextHandler(\n",
    "    model_name=\"llavanext\",\n",
    "    model_size=\"7b\",\n",
    "    model_path=MODEL_PATH,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "image_processor = ImageProcessor()\n",
    "\n",
    "# 评测类\n",
    "EvalClass = get_evaluation(SETTING)\n",
    "evaluator = EvalClass(model_handler, image_processor, DATASET_PATH, DATA_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_entry['folder']: /validation/image/mixed-images/bbox/ADE/ADE_val_00000285.png\n",
      "Image path: /home/baojiedama/ROPE/validation/image/mixed-images/bbox/ADE/ADE_val_00000285.png\n",
      "PROMPT: Given the classes: plant, window, glass, windshield, vase, mirror, tree, ceiling, cabinet, rock, person, bag, chair, door, light, food, arm, base, bottle, brand, grass, box, pole, license plate, curtain, plate, mountain, table, head, building, balcony, shelf, pillow, column, shutter, flowerpot, leg, apron, sign, picture, cushion, flower, drawer, wheel, roof, book, price tag, car, rim, handle. There are five red bounding boxes in this image. For each object within the red bounding boxes, identify its class from the list. Provide the class names in the format: 'obj1: <class1>, obj2: <class2>, obj3: <class3>, obj4: <class4>, obj5: <class5>', with no additional words or punctuation. For example: obj1: class, obj2: class, obj3: class, obj4: class, obj5: class. Replace class with the actual names of the classes from your class list. Ensure that no placeholders or brackets are used around the class names and that no additional words or punctuation are added to the response.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 4. 调用模型，打印输出\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROMPT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt)\n\u001b[0;32m---> 27\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODEL OUTPUT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 5. 如果有后处理（如分割/正则），也可以打印\u001b[39;00m\n",
      "File \u001b[0;32m/home/baojiedama/moh/model_handler.py:190\u001b[0m, in \u001b[0;36mLlavaHFHandler.generate_response\u001b[0;34m(self, prompt, image)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt, image):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# 强制用llava官方格式\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# https://huggingface.co/llava-hf/llava-1.5-7b-hf#inference-code\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    188\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m prompt}\n\u001b[1;32m    189\u001b[0m     ]\n\u001b[0;32m--> 190\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(text\u001b[38;5;241m=\u001b[39mtext, images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    196\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "File \u001b[0;32m/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1632\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1630\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1632\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(conversation, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(conversation[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conversation[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1636\u001b[0m ):\n\u001b[1;32m   1637\u001b[0m     conversations \u001b[38;5;241m=\u001b[39m conversation\n",
      "File \u001b[0;32m/home/baojiedama/miniconda3/envs/rope/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1754\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1752\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1755\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1756\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1757\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1758\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1759\u001b[0m         )\n\u001b[1;32m   1761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "DATA_JSON = \"/home/baojiedama/ROPE/validation/merged_mixed_data.json\"\n",
    "DATASET_PATH = \"/home/baojiedama/ROPE\"\n",
    "\n",
    "\n",
    "with open(DATA_JSON, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "# dataset通常是list或dict，适配下格式\n",
    "first_entry = dataset[0] if isinstance(dataset, list) else list(dataset.values())[0][0]\n",
    "# 1. 读取一条数据（merged_mixed_data.json）\n",
    "print(\"first_entry['folder']:\", first_entry[\"folder\"])\n",
    "# 2. 构造一条真实prompt\n",
    "prompt = evaluator._generate_prompt(first_entry.get(\"data_source\", \"\"))\n",
    "\n",
    "# 3. 读入图像\n",
    "folder = first_entry[\"folder\"]\n",
    "if folder.startswith(\"/\"):\n",
    "    folder = folder[1:]  # 去掉头部的斜杠\n",
    "image_path = os.path.join(DATASET_PATH, folder).replace(\"jpg\", \"png\")\n",
    "print(\"Image path:\", image_path)\n",
    "image = Image.open(image_path)\n",
    "processed_image = image_processor.preprocess_image(image)\n",
    "\n",
    "# 4. 调用模型，打印输出\n",
    "print(\"PROMPT:\", prompt)\n",
    "output = model_handler.generate_response(prompt, processed_image)\n",
    "print(\"MODEL OUTPUT:\", output)\n",
    "\n",
    "# 5. 如果有后处理（如分割/正则），也可以打印\n",
    "try:\n",
    "    segmented = evaluator.segment_classes(output)\n",
    "    print(\"SEGMENTED CLASSES:\", segmented)\n",
    "except Exception as e:\n",
    "    print(\"SEGMENT FAILED:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 加载数据 ==========\n",
    "import json\n",
    "with open(DATA_JSON, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "acc_list = [0] * 5\n",
    "photo2answer = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 主评测循环 ==========\n",
    "for entry in dataset:\n",
    "    evaluator.process_entry(entry, acc_list, photo2answer)\n",
    "\n",
    "# ========== 评测指标输出 ==========\n",
    "metrics = evaluator.calculate_metrics(acc_list)\n",
    "import pprint\n",
    "pprint.pprint(metrics)\n",
    "\n",
    "# ========== 保存结果 ==========\n",
    "with open(\"moh_llava_results.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "with open(\"moh_llava_pred.json\", \"w\") as f:\n",
    "    json.dump(photo2answer, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
